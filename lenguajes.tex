% !TeX TS-program = 
\documentclass[tesis.tex]{subfiles}

\newcommand{\ic}{independiente de contexto }
\newcommand{\APND}{automáta de pila no determinístico }
\newcommand{\APD}{automáta de pila determinístico }
\newcommand{\gramatica}{{\cal G} = (V, \Sigma, P, S)}
\newcommand{\deriva}{\overset{*}{\Rightarrow_{\cal G}}}
\newcommand{\lengderivado}{L({\cal G})}
\newcommand{\fg}{grupo finitamente generado }

\begin{document}
\chapter{Teoría de lenguajes.}	
En esta sección vamos a introducir los elementos básicos de la teoría de lenguajes formales que utilizaremos en este trabajo. 

Consideremos un conjunto no vacío $\Sigma$ que llamaremos el \blue{alfabeto} y $\Sigma^k$ el conjunto de sucesiones finitas de elementos $a_1 \dots a_k$ con  $a_i \in \Sigma$. Los elementos de $\Sigma$ se llaman \blue{letras} y los elementos de $\Sigma^k$ serán \blue{palabras} de longitud $k$ sobre $\Sigma$. La \blue{palabra vacía} que corresponde a $\Sigma^0$ la denotaremos por $\lambda$.


Si $w$ es una palabra sobre el alfabeto $\Sigma$ luego una subpalabra $u$ de $w$ es una palabra $u \in \Sigma^*$ tal que $w = vuz$ para algunas $v, z \in \Sigma^*$. Si $w = vu$ entonces $v$ es un prefijo de $w$ y $u$ es un subfijo de $w$.
\begin{deff}
	El \blue{monoide libre} sobre un alfabeto $\Sigma$ es el siguiente conjunto
	\begin{equation*}
	\Sigma^{*} = \bigcup_{k=0}^{\infty} \Sigma^k
	\end{equation*}
	con la operación $\cdot$ que es la concatenación de palabras. Es decir dadas $w_1 \in \Sigma^{k}, w_2 \in \Sigma^{l}$ luego $w_1 \cdot w_2 \in \Sigma^{k+l} \subset \Sigma^*$. El elemento neutro es la palabra vacía que corresponde a la copia de $\Sigma^0$ que es la única palabra sin letras. 
\end{deff}
\begin{obs}
	El monoide es libre con la siguiente propiedad: si tenemos una función del alfabeto $f: \Sigma \to M$ donde $M$ es algún monoide entonces existe un único morfismo de monoides $\overline f: \Sigma^{*} \to M$ que hace conmutar al siguiente diagrama.	
	
	\begin{center}
	\begin{tikzcd}
		\Sigma \arrow[r, "f"] \arrow[d, hook] & M \\
		\Sigma^* \arrow[ru, "\overline f",dashed,swap]    &  
	\end{tikzcd}
	\end{center}
	
\end{obs}


\begin{deff}
	Un \blue{lenguaje} sobre un alfabeto $\Sigma$ es un subconjunto de $\Sigma^*$.
\end{deff}



\begin{center}
	\section{Gramáticas.}
\end{center}
Vamos a considerar lenguajes definidos a partir de lo que se conoce como una gramática. Esto es esencialmente un conjunto de reglas que al irse aplicando nos permiten generar todas las palabras del lenguaje.

\begin{deff}
	Una \blue{gramática} es una tupla ${\cal G} = (V, \Sigma, P, S)$ donde:
	\begin{itemize}
		\item $V$ es un conjunto finito de \emph{variables};
		\item $S \in V$ es el \emph{símbolo inicial};
		\item $\Sigma$ es un conjunto finito de \emph{símbolos terminales} que lo tomamos disjunto de $V$;
		\item $P \subseteq (V \cup \Sigma)^*V(V \cup \Sigma)^* \times (V \cup \Sigma)^*$ es un conjunto finito de \emph{producciones}.
	\end{itemize}
\end{deff}

Dada una gramática ${\cal G} = (V, \Sigma, P, S)$ a cualquiera de sus producciones $(\gamma, \nu) \in P$, la vamos a denotar $\gamma \to \nu$. 

A partir de una gramática $\cal G$ podemos definirnos una relación sobre las cadenas $(\Sigma \cup V)^*$. 
Dados $x,y \in (\Sigma \cup V)^*$ diremos que $x$ \emph{deriva} en $y$ si existen $u,v,w,z \in (\Sigma \cup V)^*$ tales que $x = uwv$ y tenemos una producción $w \to z \in P$ de manera que $y=uzv$.
La notación que usaremos es $x \Rightarrow_{\cal G} y$. 
Consideremos la clausura transitiva y reflexiva de esta relación que denotaremos por $\deriva$.

%Dados $uAv \in (\Sigma \cup V)^* V(\Sigma \cup V)^*$ y $w \in (\Sigma \cup V)^*$ si tenemos alguna producción $uAv \to w$ diremos que $uAv$ \emph{deriva} en $w$ y lo denotaremos $uAv \Rightarrow_{\cal G} uwv $. 
%Esto nos define una relación $\Rightarrow$ sobre $(\Sigma \cup V)^*$. 
%Tomamos la clausura transitiva, simétrica y reflexiva para así obtener una relación de equivalencia que denotaremos $\deriva$.
 

\begin{deff}
	El \blue{lenguaje generado por la gramática} van a ser las palabras en $\Sigma^*$ que se pueden derivar del símbolo inicial $S$. Formalmente esto es
	\[
	L({\cal G}) = \{ w \in \Sigma^* \ | \ S \overset{*}{\Rightarrow_{\cal G}} w   \}.
	\]
\end{deff}
\medskip
\begin{ej}\label{gramatica-regular}
	Consideremos la siguiente gramática ${\cal G} = (V, \Sigma, P, S)$ donde $V = \{ S, A \}, \Sigma = \{ a,b \}$ y tenemos las siguientes producciones,
	\begin{align*}
	S & \to Ab \\
	A & \to aA \\
	A & \to \lambda
	\end{align*}
	Veamos como podemos derivar la palabra $a^2b$ usando las producciones de esta gramática. 
	Esto es que $S \deriva a^2b$.
	Tomamos la siguiente sucesión:
	\begin{align*}
		S \to Ab \to aAb \to aaAb \to aab
	\end{align*} 
	y nos queda tal como queríamos ver.
	
	Más aún probemos que $L({\cal G}) = \{ a^{k}b : k \ge 0 \}$. 
	
	Si $w \in L(\cal G)$ entonces $S \deriva w$ por definición. 
	La única producción que la gramática tiene donde $S$ está a la izquierda es $S \to Ab$. 
	De esta manera cualquier palabra $w \in L(\cal G)$ va a tener una $b$ como postfijo de la palabra. 
	La variable $A$ vemos que solo puede derivar en $a^{k}A$ o $a^{k}$ para $k \ge 0$.
	Esto se debe a que podemos aplicar la producción $A \to aA$ tantas veces como querramos por lo tanto $A \deriva a^k$ para cualquier $k \ge 0$.
	Juntando con lo anterior vemos que $S \deriva a^{k}b$ así que terminamos de ver que la gramática genera al lenguaje $\{a^kb : k \ge 0\}$ tal como queríamos ver.
\end{ej}



Es posible clasificar los lenguajes a partir de las características de las gramáticas que los generan. 

\section{Lenguajes regulares.}

\begin{deff}
	Decimos que una gramática $\gramatica$ es \blue{ regular} si las producciones son del estilo
	\begin{enumerate}
		\item $A \to \lambda$
		\item $A \to a$
		\item $A \to a B$
	\end{enumerate}
	donde $A, B \in V$, $a \in \Sigma$ y $\lambda$ es la palabra vacía. 
	Si $L=\lengderivado$ para alguna gramática regular $\cal G$ entonces diremos que $L$ es un \blue{lenguaje regular}. 
\end{deff}

En particular la gramática del ejemplo \ref{gramatica-regular} es regular. 
De esta manera $L= \{ a^k b : \ k \ge 0  \}$ resulta ser un lenguaje regular.


\begin{deff}
	Un \blue{autómata finito no determinístico} es una tupla ${\cal M} = (Q,\{q_0\},\Sigma,\delta,F)$ donde:
	\begin{itemize}
	\item $Q$ es el conjunto de los \emph{estados}.
	\item $q_0 \in Q$ es el \emph{estado inicial}.
	\item $\Sigma$ es el \emph{alfabeto}.
	\item $\delta:Q \times \Sigma \to {\cal P}(Q)$ es la \emph{función de transición}.
	\item $F \subseteq Q$ es un subconjunto de estados que llamaremos \emph{finales}.
	\end{itemize}
\end{deff}

A los automátas finitos los interpretamos como grafos con algunas reglas para movernos sobre ellos. 
Para empezar $Q$ representan los vértices de nuestro grafo.

\begin{ej}
	Construyamos un automáta $\cal M$ tal que acepte al lenguaje $L = \{ a^{k}b \ : \ k \ge 0  \}$.
	
\end{ej}


Como vimos en este ejemplo el lenguaje $L = \{ a^kb \ : \ k \ge 0  \}$ es aceptado por un automáta no determinístico finito y por lo que sabíamos del ejemplo \ref{gramatica-regular} es un lenguaje regular.
Más aún vale que los lenguajes aceptados por automátas finitos no determinísticos son justamente los regulares.

\begin{teo}
 Un lenguaje $L$ es regular sii es aceptado por un autómata finito no determínistico.
\end{teo}

\begin{proof}
	Demostración estándar. Ver \cite{hopcraft-ullman}.
\end{proof}




%definición automáta finito
%definición de lenguaje regular por medio del automáta, las otras defs no son necesarias
% comentar que los regulares son cerrados para grupos? quizá pueda servir para después





\section{Lenguajes independientes de contexto.}
\begin{deff}
	Decimos que una gramática $\gramatica $ es \blue{independiente de contexto} si las producciones tienen la siguiente forma:
	\begin{equation*}
	A \to w
	\end{equation*}
	donde $A \in V, w \in (\Sigma \cup V)^*$.  
	Si $L=\lengderivado$ para alguna gramática independiente de contexto $\cal G$ entonces diremos que $L$ es un \blue{lenguaje independiente de contexto}.
\end{deff}


\begin{ej}
	Sea el alfabeto $\Sigma = \{ a,b \}$. Si $w=a_1 \dots a_k$ es una palabra sobre $\Sigma^*$ entonces podemos considerar a $w^r$ que es la palabra inversa dada por leerla de derecha a izquierda y tiene la siguiente pinta $w^r= a_k \dots a_1$. 
	Consideremos sobre $\Sigma$ el lenguaje $L = \{ w \in \Sigma^* \ | \ w = w^r  \}$ tal que este es el lenguaje de los palíndromos. 
	Construyamos una gramática independiente de contexto para este lenguaje.
	Sea $w$ una palabra en $L$ luego sabemos que si su longitud es mayor que uno entonces debe ser que $w = a u a$ o $w = b u b$ para cierta palabra $u \in L$ dado que $u$ necesariamente tiene que ser un palíndromo porque $w$ lo es. 
	Esto sucede para todas las palabras del lenguaje exceptuando las palabras de longitud uno que son justamente las letras del alfabeto. 
	De esta manera podemos considerar la siguiente gramática ${\cal G}  =  (\{S\}, \Sigma, P, S )$ donde las producciones $P$ están dadas por :
	\begin{align*}
	S  & \to \epsilon \\ S &\to a \\ S &\to b \\ S &\to  aSa \\ S &\to bSb. \\
	\end{align*}
	Para ver que esta gramática genera al lenguaje $L$ notemos que si tomamos una palabra en el lenguaje $w \in L$ luego si no es una letra al ser palíndromo sobre el alfabeto $\Sigma$ necesariamente debe comenzar con $a$ o con $b$ y de esta manera tenemos que la primer derivación es $S \to aSa$ o $S \to bSb$. 
	Dado que la subpalabra $w = aua$ que se obtiene de $w$ sin considerar la primera y última letra es un palíndromo (e incluso podría ser la palabra vacía) luego podemos repetir este proceso para llegar a $S \deriva w$ después de finitos pasos. 
	Por otro lado toda palabra generada por esta gramática es un palíndromo porque todas las reglas son tales que agregan una letra al principio y la misma al final o simplemente agregan letras que también son palíndromos.
	
\end{ej}

Toda gramática independiente de contexto la podemos tomar para que sea de una forma en particular.

\begin{deff}
	Una gramática $\gramatica$ independiente de contexto está en su \blue{forma normal de Chomsky} si las producciones son de este tipo:
	\begin{enumerate}
		\item[\textbf{CH1.}] $A \to BC$ donde $A\in V$ y $B,C \in V \setminus \{ S \}$.
		\item[\textbf{CH2.}] $A \to a$ donde $A \in V, a \in \Sigma$.
		\item[\textbf{CH3.}] $S \to \lambda$ 
	\end{enumerate}
\end{deff}



\begin{prop}
	Para toda gramática $\cal G$ independiente de contexto puede tomar otra $\cal G'$ tal que esté en forma normal de Chomsky y $\lengderivado = L(\cal G')$,
\end{prop}

\begin{proof}
	Demostración estándar. Ver \cite{hopcraft-ullman}.
\end{proof}

\subsection{Autómatas de pila.}
Así como las gramáticas nos permiten generar un lenguaje tenemos las máquinas que nos permiten aceptar un lenguaje. En nuestro caso en particular vamos a usar autómatas de pila no determinísticos para aceptar los lenguajes independientes de contexto.

\begin{deff}
	Un \blue{autómata de pila finito no determinístico} es una tupla 
	\[
	{\cal M } = (Q, \Sigma, Z, \delta, q_0, F, Z_0)
	\]
	 donde:
	\begin{itemize}
		\item $Q$ es un conjunto finito de \emph{estados};
		\item $\Sigma$ es un conjunto finito que denotaremos el \emph{alfabeto del lenguaje};
		\item $\Gamma$ es un conjunto finito que denotaremos el \emph{alfabeto de la pila};
		\item $\delta$ es la \emph{función de transición} donde $\delta: Q  \times \Sigma \times \Gamma \to {\cal P}( Q  \times \Gamma^*)$;
		\item $F \subseteq Q$ es el conjunto de \emph{estados finales};
		\item $q_0 \in Q$ es el \emph{estado inicial};
		\item $Z_0 \in Z$ es el \emph{símbolo inicial} en la pila.
	\end{itemize}
\end{deff}


\paragraph{Funcionamiento del autómata.}
%Un prefijo de $w$ será una subpalabra $\gamma$ tal que $w=\gamma w'$ visto con la concatenación de palabras en $\Sigma^*$.

El autómata de pila finito funciona de la siguiente manera. 
Dada una palabra $w \in \Sigma$ queremos saber si es aceptada por el autómata de pila o no. Para eso vamos a ir leyendo esta palabra de izquierda a derecha. 
Al comenzar a leer esta palabra estamos en el estado $q_0$ que distinguimos como el estado inicial de nuestro autómata. 
Nos fijamos en la función de transición que es una función parcial cuánto nos da evaluada  $\delta(\lambda,q_0,\gamma)$ para algún $\gamma$ prefijo de $w$ y donde estamos mirando a $\lambda \in Z^*$ el elemento neutro de este monoide. 
Esto se corresponde a la idea de que al comenzar nuestra pila está vacía. 
En tal caso nuestra función de transición nos da un resultado que es un par $(z,q)$ donde $z \in Z^{*}$ es lo que nos va a quedar en la pila y $q$ es el nuevo estado al cual nos movimos. 
Notemos que nuestra función de transición no tiene porqué tener un $(z,q)$ tal que podamos movernos o podría ser bien que tenga más de uno. 
En este caso diremos que nuestro autómata es \blue{no determinístico} dado que en algunos casos existe más de una opción.

En general estamos en la siguiente situación en el proceso de aceptar la palabra $w$. 
Tenemos algún $z \in Z$ en el tope de la pila que al ser una pila lo leeremos de derecha a izquierda, estamos en algún estado $q \in Q$ y nos quedará una subpalabra $\gamma$ de $w$ para leer. Una \blue{configuración} de nuestro autómata entonces es una manera de describir en que situación de aceptar o no aceptar una palabra y la denotamos $(z,q,\gamma)$. 

Cuando hayamos visto toda la palabra o no tengamos manera de movernos de estado nos fijamos si el estado $p$ en el que estamos es final, es decir si $p \in F$. 
En tal caso la palabra $w$ es aceptada por el autómata. Formalmente estaremos en alguna configuración $zq$ para $z \in Z^*$ y $q \in Q$ y no nos queda nada de la palabra $w$ porque ya la consumimos toda.
\medskip
\begin{obs}
	Así como definimos el \APND para que al tener la pila vacía se detenga podríamos haberlo hecho de una manera distinta por ejemplo dejando que la pila sea vacía y así poder transicionar de una configuración a otra.	
	Esta manera es equivalente porque...
	Más en adelante nos va a resultar conceptualmente más útil para el caso que estemos viendo el problema de la palabra de un grupo mientras que para las demostraciones de esta sección usaremos esta otra definición.
\end{obs}

\paragraph{Descripción instantánea del autómata.} Veamos ahora como describir formalmente el funcionamiento de un autómata a partir de lo que estamos haciendo en cierto instante. 
Consideremos que estamos en el instante que nuestra subpalabra que nos queda por leer es $w$, estamos en un estado $q$ y en nuestra pila tenemos la palabra $\gamma$, entonces vamos a representar al instante por medio de esta tupla $(q,w,\gamma)$.
Si ahora tenemos la posibilidad de movernos a otro estado $p$ tal que $(p,w',\alpha\beta) \in \delta (q,aw',x\beta)$ donde $aw' = w$ con $a$ alguna letra posiblemente vacía y similarmente $x \beta = \gamma$ con $x \in \Gamma^*$ una letra de $\Gamma$ posiblemente vacía. 
Este movimiento lo denotamos como $(q,aw',x\beta) \vdash (p,w',\alpha \beta)$. 
Podemos considerar la clausura transitiva de esta relación sobre los triples $Z \times Q \times \Sigma^*$ que denotaremos $\vdash^*$.


\paragraph{El lenguaje aceptado por un autómata de pila.} 
Notemos que en particular el autómata de pila nos da un lenguaje que está formado por las palabras $w$ en el alfabeto de la entrada del autómata $\Sigma$ que son aceptadas. 
En general diremos que un autómata acepta un lenguaje $L$ si su lenguaje aceptado es exactamente $L$. 
Este lenguaje aceptado por el autómata $\cal M$ en algunas casos para hacer énfasis en el autómata lo denotaremos $L {(\cal M)}$ y siguiendo la notación formal anterior lo podemos describir de la siguiente manera,
\begin{equation*}
	L( {\cal M}) = \{ w \in \Sigma^* \ | \ (q_0,w,Z_0) \vdash^* (q, \epsilon, \gamma), \ \ q \in F, \ \gamma \in \Gamma^*      \}
\end{equation*}
donde $(q_0, w, Z_0)$ es el instante inicial en el cual tenemos la palabra $w$ en el estado inicial $q_0$ con el símbolo $Z_0$ de la pila. Al finalizar deberíamos estar en un estado $q$ final y lo que nos queda en la pila $\gamma$ es totalmente irrelevante en este caso.

\begin{obs}
	Al ser un autómata de pila no determinístico existen posiblemente más de una manera de consumir alguna palabra en el autómata. 
	Es así que por la definición que dimos la palabra es aceptada por el autómata si al menos alguna de estas derivaciones la lleva a ser aceptada.
	No importa si existen derivaciones que no lo hagan mientras una sí lo haga. 
\end{obs}
\begin{ej}
	Sea nuestro alfabeto $\Sigma = \{ a, b\}$ y $L$ el lenguaje de los palíndromos sobre este alfabeto. Consideremos el siguiente autómata de pila 
	\[
	M=(Q,\{q_0\} ,\{a,b\}, \{a,b,\$\}, \$, \{q_1\})
	\]
	donde nuestra pila tiene el mismo alfabeto que el de entrada con un símbolo extra que es \$ que va a ser nuestro símbolo inicial de la pila.
	\begin{center}
		\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3.5cm,
		scale = 1,transform shape]
		
		\node[state,initial] (q_0) {$q_0$};
		\node[state,accepting] (q_1) [right of=q_0] {$q_1$};
		
		\path (q_0) edge [bend right]             node {} (q_1);
		\path (q_1) edge [loop above] node {}    (   );
		\path (q_0) edge [loop above] node {}    (   );
		\end{tikzpicture}
	\end{center}
	
	
	El autómata tiene dos estados, el inicial y el final. La idea es que en el primer estado vamos apilando la palabra y en la segunda vamos desapilando la palabra anteriormente apilada. Nuestra función de transición va a ser la siguiente,
	\begin{itemize}
		\item $\delta(q_0,a,Z) = (q_0,aZ)$ 
		\item $\delta(q_0,b,Z) = (q_0,ba)$ 
		\item $\delta(q_1,a,a) = (q_0,\lambda)$.
		\item $\delta(q_1,b,b) = (q_0,\lambda)$.
		\item $\delta (q_0, \lambda, Z) = (q_1,Z)$
		\item $\delta (q_0, X, Z) = (q_1, Z)$ 
	\end{itemize}
	donde $Z=a,b, \$$ es decir cualquier cosa del alfabeto de la pila y $X = a, b$ cualquier elemento de nuestro alfabeto de entrada. 
	El autómata en el primer estado apila lo que sea que estemos leyendo sin importar lo que esté en el tope de pila. 
	En el segundo estado desapila cada vez que lo que estemos leyendo coincida con el tope de pila. 
	Finalmente para ir del estado inicial al final tenemos en cuenta dos casos. Nos podemos mover por $\lambda$ es decir sin consumir ninguna letra de la palabra de la entrada o leyendo alguna de las letras de la palabra. Estos casos se corresponden a que el palíndromo tenga longitud par o tenga longitud impar.	
\end{ej}

Este ejemplo nos da un indicio que los lenguajes aceptados por autómatas de pila también podrían ser \ic y esto efectivamente es cierto.

\medskip

\begin{teo}\label{teo_ic_apnd}
Un lenguaje $L$ es independiente de contexto sii es aceptado por un autómata de pila no determinístico.
\end{teo}

\begin{proof}
	%Demo importante diría si bien es super estándar y conocidísimo el resultado.
	Ver \cite{hopcraft-ullman}.
\end{proof}
 


Hasta ahora definimos los autómatas de pila no determinísticos que aceptan por estado final. Otra definición posible de lenguaje aceptado podría ser que acepten por pila vacía. 
Es decir que una vez que consumimos la palabra $w$ de entrada llegamos a una configuración $(q, \lambda, \lambda)$ donde $\lambda$ es la palabra vacía de ambos alfabetos respectivamente. Formalmente notaremos al lenguaje aceptado por pila vacía por un autómata $\cal M$
de la siguiente manera,
\begin{equation*}
	L({\cal M}) = \{ w \in \Sigma^* \ | \ (q_0,w,Z_0) \vdash^* (q, \lambda, \lambda), \ q \in Q, u \in \Sigma^*    \}
\end{equation*}
donde arrancamos en el estado inicial y llegamos a algún estado $q$ cualesquiera y nuestra pila está vacía así como lo que nos queda por leer de la palabra. 
En este caso obtenemos que nuestro lenguaje es aceptado por un autómata de pila no determinístico por pila vacía.


El siguiente resultado nos dice que en el caso que nuestro autómata sea no determinístico es equivalente usar una u otra manera de definir a nuestro lenguaje.

\medskip
\begin{teo}
Un lenguaje $L$ es aceptado por un autómata de pila no determinístico por estado final sii es aceptado por un autómata de pila no determinístico por pila vacía.
\end{teo}

\begin{proof}
	%Demo bastante fácil si se hacen los dibujitos. Quizá ni haga falta escribirla.
	Ver \cite{hopcraft-ullman}.
\end{proof}


\subsection{Propiedades de los lenguajes independientes de contexto.}

En esta sección vamos a considerar a los lenguajes \ic como una clase. 
Probemos algunas propiedades que cumplen respecto a las operaciones de conjuntos más usuales.

\begin{prop}
	Los lenguajes \ic son cerrados por uniones.
\end{prop}
\begin{proof}
	% Demo fácil armandonos la gramática correspondiente. No sé si hace falta agregarlo igual.
	Ver \cite{hopcraft-ullman}.
\end{proof}

\begin{prop}
	Los lenguajes independientes de contexto no son cerrados por intersecciones.
\end{prop}

\begin{proof}
	%Contraejemplo clásico. No sé si hace falta poner esto porque no lo voy a usar.
	Ver \cite{hopcraft-ullman}.
\end{proof}

Lo que si sucede es que son cerrados con respecto a intersecciones con lenguajes regulares.

\begin{prop}\label{intersecciones-reg-ic}
	Los lenguajes \ic son cerrados por intersecciones con lenguajes regulares.
\end{prop}

\begin{proof}
	% Demo no tan cortita pero está buena porque se ven como usar los dos tipos de automátas.
	Ver \cite{hopcraft-ullman}.
\end{proof}

Otras propiedades interesantes tienen que ver con su relación con morfismos de monoides. 

\begin{prop}\label{morfismos-monoides-ic}
	Los lenguajes \ic son cerrados por:
	\begin{enumerate}
		\item Imágenes de morfismos de monoides.
		\item Preimágenes de morfismos de monoides.
	\end{enumerate}
\end{prop}
\todo[inline]{Estaría bueno agregarlo por completitud porque no se ve en los cursos usuales y es necesario para definir los grupos IC.}
\begin{proof}
	%Escribir las dos demos. La segunda no es tan fácil ni evidente.
	Ver \cite{sipser13}.
\end{proof}

Finalmente la herramienta principal que tenemos para ver que cierto lenguaje $L$ no es \ic es usar el siguiente lema.

\begin{lema}[Pumping] \label{pumping}
	Sea $L$ un lenguaje independiente de contexto entonces existe una constante $n \ge 0$ tal que para todas las palabras $w \in L$ de longitud al menos $n$ existe una factorización $w = uvxwy$ con $|vwx| \le n$ y $|vx| > 0$ tal que para todo $i \in \NN$ vale que $uv^iwx^iy \in L$.
\end{lema}

\begin{proof}
	Resultado estándar. Ver \cite{hopcraft-ullman}.
\end{proof}

\section{Autómatas de pila determinísticos.} Si en la definición anterior del autómata pedimos que la función de transición de una configuración dada tenga a lo sumo un valor entonces nuestro autómata lo vamos a llamar determinístico. Formalmente esto es que 
\[
|\delta(q,a, z)| \le 1 \ \ \ \forall z \in Z, \ p \in Q, \ a \in \Sigma \cup \{ \lambda \}.
\]
En cierta manera estamos diciendo que de un instante dado solo tenemos a lo sumo una única posibilidad de movernos a otro estado. A los lenguajes aceptados por un \APD los pensamos que son aceptados por un estado final.


 Por lo tanto para cada palabra tenemos un único camino en el autómata para saber si es aceptada o no a diferencia de un \APND  que podría tener varios caminos posibles para cada palabra.
\begin{obs}
	Todo \APD en particular es no determinístico y por lo tanto la clase de lenguajes aceptados por los primeros están contenidos en la clase de los segundos.
\end{obs}

Veamos que esta contención es estricta. Para eso volvamos a considerar el ejemplo del lenguaje de los palíndromos.
\begin{ej}
	El lenguaje $L = \{ w \in \{ a,b \}^*  \ : \ w = w^r \}$ no es aceptado por un \APD pero sí por uno no determinístico. 
	Supongamos que $M$ es un \APD que lo acepta. 
	Notemos que para cualquier palabra $w \in \{ a,b \}^*$ debe ser que al consumirla la pila no puede quedar vacía dado que $ww^r \in L$ y en tal caso no aceptaría a esta palabra. 
	Esto es que la pila nunca está vacía sea cual sea la configuración que lleguemos. 
	Para cada palabra arbitraria $w$ existe otra $x_w$ tal que al procesar $wx_w$ lo que nos queda en la pila es de tamaño mínimo con respecto a todas las palabras $wx$. 
	Sea entonces lo que tiene en la pila la palabra $\alpha_w$ que sabemos es de longitud mínima. Si consideramos palabras del estilo $wx_wz$ sabemos que la longitud de lo que quede en la pila no puede disminuir. 
	Ahora consideremos dos palabras del estilo $r=wx_w, s=uy_u$ tales que sus pilas son de longitud mínima al terminar de recorrer las palabras y que resultan tener el mismo tope de pila y terminar en el mismo estado. 
	Podemos asegurar la existencia de estas palabras debido a que tenemos finitos estados y combinaciones de tope de pilas dado que el autómata de pila es finito pero tenemos infinitas palabras que cumplen esta propiedad. 
	Ahora basta con elegir $z$ de modo que $tz$ sea palíndromo pero que $sz$ no lo sea. 
	
	Veamos que podemos elegir a $z$ para que una de las concatenaciones $tz,sz$ sea palíndromo y la otra no. Partamos en distintos casos. 
	Si $|t|=|s|$ basta con tomar $z=s^r$. 
	Si $|t|\neq |s|$ y supongamos que $s$ tiene longitud menor y no es prefijo de $t$ entonces de nuevo podemos tomar el palíndromo $ss^r$ tal que $ts^r$ no es un palíndromo. 
	Finalmente queda el caso que una es un prefijo de la otra, supongamos $t=su$. 
	Si elegimos $x=a,b$ tal que $ux$ no sea un palíndromo luego la palabra $ss^r$ es un palíndromo pero $suxs^r$ no lo es.
	
	Esto muestra que si bien $sz \notin L$ y $tz \in L$ el \APD no va a poder diferenciarlas por lo tanto no es posible que este lenguaje sea aceptado por un \APD tal como queríamos ver.
	
\end{ej}

\subsection{Autómatas de pila determinísticos especiales.} 
Consideremos ahora un \APD tal que acepta tanto por estado final como por pila vacía. A estos los llamaremos \textit{autómatas de pila determinístico especiales}.  
Estos autómatas son los que nos surgen de la construcción del autómata del problema de la palabra  para grupos virtualmente libres. 

\begin{ej}
	Sea el lenguaje $L = \{ a^m b^n  : m \ge n \ge 1 \}$ este no es un lenguaje independiente de contexto determinístico especial pero sí es determinístico. 
	
	Construyamos un \APD que acepte a $L$. Sea $$M = (\{q_0,q_1,q_2\}, \{q_0\}, \{a,b\}, \{a,b,Z_0\}, Z_0, q_2) $$ el siguiente \APD que representamos así:
	
	\begin{center}
		\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3.5cm,
		scale = 1,transform shape]
		
		\node[state,initial] (q_0) {$q_0$};
		\node[state] (q_1) [right of=q_0] {$q_1$};
		\node[state,accepting] (q_2) [right of=q_1] {$q_2$};
		
		\path (q_0) edge    [bend right]          node {$b, a | \lambda$} (q_1)
		(q_0) edge    [loop above]          node {$a, Z | aZ$} (q_0)
		(q_1) edge      [bend right]      node {$\lambda, a | Z$}   (q_2)
		(q_1) edge    [loop above]           node {$b, a | \lambda$} (q_1);
		
		\end{tikzpicture}
	\end{center}
	
	
	
	donde $Z$ es cualquier letra del alfabeto de la pila. El autómata en el estado inicial $q_0$ apila a todas las $a$ y cambia al estado $q_1$ cuando lee por primera vez una $b$ y en ese caso desapila la a que está en el tope de la pila. En el estado $q_1$ sigue desapilando cada vez que ve una $b$. Finalmente va al estado $q_2$ cuando en la pila sigue quedando $a$ y ya leímos toda la palabra y en tal caso la acepta porque significa que vimos como máximo tantas $b$ como $a$ y este es el lenguaje que buscábamos generar.
	
	El lenguaje no es aceptado por un \APD por pila vacía dado que tiene la propiedad de los prefijos. 
	Es decir que existen palabras que están en el lenguaje tales que alguno de sus prefijos también están. 
	Por ejemplo consideremos $a^m b^i$ y  $a^m b^j$ para $m \ge 2$ e $i < j \le m$. 
	Esto es porque si $M = (Q, \Sigma, Z, Z_0, \delta, q_0 , F)$ fuera un \APD que acepta por pila vacía a este lenguaje tendríamos que $(q_0,a^{m}b^{i},Z_0) \vdash^* (q,\lambda, \lambda)$ con $q$ un estado final pero como tiene la pila vacía no podemos continuar aceptando a la palabra $a^mb^j$ ya que por la definición que empleamos el autómata necesita leer algún elemento de la pila. 
	De esta manera vemos que este lenguaje no puede ser aceptado por pila vacía y estado final por un \APD concluyendo que los lenguajes determinísticos especiales forman un subfamilia propia de los independientes de contexto determinísticos.
	
	\begin{comment}[Esto es con la otra definición de automáta de pila.]
			Por el principio del palomar podemos ver que este lenguaje no es aceptado por \APD por pila vacía y estado final a la vez. Si así lo fuera supongamos que existe un automáta que lo acepta $M = (Q, \Sigma, Z, Z_0, \delta, q_0 , F)$. Debido a que tiene finitos estados podemos elegirnos $m$ suficientemente grande tal que existan palabras $a^mb^j, a^mb^i$ con $1 \le i < j < m$ y elegidas de manera que terminen en el mismo estado final $p$. Notemos que $(q_0,a^mb^ib^{m-j+1},Z_0) \vdash^* (q,\lambda, Z)$ donde $q$ es un estado final dado que $m-j+1+i \le m$. Por otro lado debe ser que $(q_0, a^mb^jb^{m-j+1}) \vdash^* (q,\lambda, Z) $ pero notemos que $a^mb^{m-1} \notin L$. Por lo tanto llegamos a una contradicción y de esta manera vemos que $L$ no es aceptado por un \APD especial tal como queríamos ver.
	\end{comment}

	
\end{ej}





\section{Lenguajes poly independientes de contexto.}

En esta sección vamos a introducir otra familia de lenguajes que generalizan levemente a los lenguajes independientes de contexto. 
La bibliografía fundamental es \cite{brough2014groups}.


\section{Conos de lenguajes y el problema de la palabra.}

En esta sección consideraremos un grupo $G$ finitamente generado por algún alfabeto $\Sigma$ finito donde lo vemos como un conjunto de generadores como monoide dado que en particular todos los grupos son monoides. 
De esta manera tenemos un epimorfismo de monoides $\pi: \Sigma^* \twoheadrightarrow  G$. 
%En esta sección todos los lenguajes $L$ considerados van a estar definidos sobre algún alfabeto finito, esto es que existe $\Sigma$ alfabeto finito tal que $L \subseteq \Sigma^*$.

El problema de la palabra es uno de los problemas de teoría de grupos más centrales al área. Explícitamente el problema consiste en dada una palabra $\omega \in \Sigma^*$ en los generadores del grupo encontrar un algoritmo para decidir si esta palabra es la identidad del grupo o no. Notemos que para poder pensar este problema estamos fijando de antemano algún conjunto de generadores posible del grupo.

Dado un grupo finitamente presentado consideramos el siguiente lenguaje 

\[ \text{WP} (G, \Sigma) = \{ \omega \in \Sigma^* \ | \ \omega \underset{G}= 1 \}\]

que llamaremos el \blue{problema de la palabra de $G$ para los generadores $\Sigma$}. 

Queremos ver sobre qué clases de lenguajes el problema de la palabra queda bien definido y no depende de los generadores elegidos.


Así como definimos los lenguajes regulares vamos a preocuparnos por otros tipos siempre y cuando cumplan las siguientes condiciones. 
\medskip
\begin{deff}
	Una clase de lenguajes $\CC$ es un \blue{cono} si para todo $L \in \CC$ resulta que:
	\begin{itemize}
		\item[\textbf{C1.}] Es cerrado por imágenes de morfismos de monoides. Sea $L \subset \Sigma^*$ luego si existe $\phi:\Sigma^* \to \Delta^*$ morfismo de monoides debe ser que $\phi(L) \in \CC$.
		\item[\textbf{C2.}] Es cerrado por preimagenes de morfismos de monoides. Sea $L \subset \Sigma^*$ luego si existe $\phi:\Sigma^* \to \Delta^*$ morfismo de monoides  debe ser que $\phi^{-1}(L) \in \CC$. 
		\item[\textbf{C3.}] Es cerrado por intersecciones con lenguajes regulares. Si $R$ es un lenguaje regular sobre $\Sigma^*$ entonces $L \cap R \in \CC$ también resulta serlo.
	\end{itemize}
\end{deff} 

\begin{ej}\label{ic-cono}
	Los lenguajes independientes de contexto forman un cono.
	Esto se puede ver a partir de las proposiciones  \ref{intersecciones-reg-ic} y \ref{morfismos-monoides-ic}.
\end{ej}

Los conos de lenguajes cumplen la siguiente propiedad de gran importancia para el estudio del problema de la palabra.
\medskip
\begin{prop}\label{prop-cono-wp}
	Sea $WP(G, \Sigma)$ el lenguaje del problema de la palabra de cierto grupo $G$ para algunos generadores $\Sigma$ y $\CC$ cono de lenguajes. 
	Si $WP(G, \Sigma) \in \CC$ luego valen las siguientes afirmaciones:
	\begin{itemize}
		\item[\textbf{W1.}] $WP(G, \Delta) \in \CC$ para cualquier conjunto de generadores $\Delta$.
		\item[\textbf{W2.}] $WP(H) \in \CC$ para todo subgrupo $H$ \fg de $G$.
	\end{itemize} 
\end{prop}
\begin{proof}
		Para ver \textbf{W1} formamos el siguiente diagrama conmutativo,
		\begin{center}
			\begin{tikzcd}
				\Delta^* \arrow[r, "\delta"] \arrow[d,"f",swap] & G \\
				\Sigma^* \arrow[ru, "\pi",swap]    &  
			\end{tikzcd}
		\end{center}
		donde $f$ es algún morfismo de monoides y donde usamos la propiedad universal de los monoides libres.
		Notemos que $WP(G, \Delta) = \delta^{-1}(1)$ y como el diagrama conmuta tenemos que 
		\[
		f^{-1}(\delta^{-1}(1)) = f^{-1}(WP(G,\Sigma)) = WP(G, \Delta).
		\]
		Dado que esto es un cono obtenemos lo que queríamos ver puesto que es cerrado por preimágenes de morfismos de monoides.
		
		
		
		Veamos ahora que vale \textbf{W2}. 
		Sea $\Sigma'$ conjunto de generadores de $H$.
		Siempre podemos extenderlo a $\Sigma$ tal que $\Sigma$ genere a $G$. 
		De esta manera 
		\[
		WP(H, \Sigma') = WP(H, \Sigma) \cap \Sigma'^*
		\]
		y de vuelta como es un cono la intersección con lenguajes regulares nos da un lenguaje en el cono. 

\end{proof}


Esto nos dice que es interesante estudiar el problema de la palabra justamente sobre conos.


\section{Sistemas de reescritura.}

Un \emph{sistema de reescritura} podemos pensarlo como un grafo en el sentido de Serre. 
En este caso los vértices son los \emph{objetos} mientras que las aristas las llamamos \emph{movimientos}. 
Si nuestro sistema de reescritura $\Gamma$ tiene un movimiento de un objeto $a$ en otro objeto $b$ diremos que $a$ puede ser \emph{reescrito} a $b$ y lo denotaremos $a \to_{\Gamma} b$, omitiendo aclarar que estamos considerando el sistema $\Gamma$ en todo caso que no sea ambiguo. 
Un camino en el grafo lo llamaremos una \emph{derivación}.  

Nosotros queremos que estos sistemas de reescritura sean tales que si tomamos una sucesión de movimientos en algún momento se estabilice y en tal caso llamaremos \emph{terminante.} 
A su vez los objetos tales que no puedan ser modificados por ningún movimiento llamaremos objetos \emph{terminales.} 

Dado un sistema de reescritura $\Gamma$ por medio de la clausura transitiva que denotaremos $\overset{*}{\rightarrow}_{\Gamma}$ obtenemos una relación transitiva sobre los objetos. 
Nuestro interés justamente va a estar en estudiar las relaciones de equivalencia que surgen de sistemas de reescritura particulares. 

\begin{ej}
	Considerar como objetos los enteros y como movimiento dividir al número por dos si es posible. Este sistema de reescritura tiene como objetos terminales los números impares.
\end{ej}

Otra característica que le vamos a pedir a los sistema de reescritura es que sean \emph{confluentes}. 
Informalmente esto es que a partir de un objeto, si usamos dos derivaciones distintas entonces eventualmente estas derivaciones se encuentran en algún objeto independientemente de qué derivaciones tomemos. 
Esto es que dadas derivaciones $a \overset{*}{\rightarrow} b, a \overset{*}{\rightarrow} c$ existe un objeto $d$ tal que $b \overset{*}{\rightarrow} d$ y $c \overset{*}{\rightarrow} d$.

En particular los sistemas que nos van a interesar en este trabajo son sistemas tales que son confluentes y terminantes que son conocidos como \emph{Church-Roser}.

\subsection{Sistemas de reescritura de cadenas.}
En nuestro caso en particular vamos a trabajar con sistemas de reescritura donde los objetos son palabras sobre algún alfabeto $\Sigma$ y los movimientos son reglas del estilo $w \to v$ que interpretamos de la siguiente manera. 
Si tenemos alguna palabra que tenga como subpalabra a $w$ podemos modificarla por $v$. 
Esto lo representamos por el triple $(p, w \to v, q)$ donde $p,q$ son las subpalabras que vienen antes de $w$ tales que pueden ser vacías. 

Al sistema de reescritura $\Gamma$ con alfabeto $\Sigma$ y las reglas $\cal R$ lo vamos a denotar por sr$\left< \Sigma, {\cal R} \right>$. Estos sistemas también son llamados \emph{sistemas de Thue}.


\end{document}

