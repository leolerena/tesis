% !TeX TS-program = 
\documentclass[tesis.tex]{subfiles}

%\newcommand{\ic}{independiente de contexto }
%\newcommand{\APND}{automáta de pila no determinístico }
%\newcommand{\APD}{automáta de pila determinístico }
%\newcommand{\gramatica}{{\cal G} = (V, \Sigma, P, S)}
%\newcommand{\deriva}{\overset{*}{\Rightarrow_{\cal G}}}
%\newcommand{\lengderivado}{L({\cal G})}
%\newcommand{\fg}{grupo finitamente generado }

\begin{document}
\chapter{Preliminares.}
\section{Teoría de lenguajes.}	
En esta sección vamos a introducir los elementos básicos de la teoría de lenguajes formales. 

Consideremos un conjunto no vacío $\Sigma$ que llamaremos el \blue{alfabeto} y $\Sigma^k$ el conjunto de sucesiones finitas de elementos $a_1 \dots a_k$ con  $a_i \in \Sigma$. Los elementos de $\Sigma$ se llaman \blue{letras} y los elementos de $\Sigma^k$ serán \blue{palabras} de longitud $k$ sobre $\Sigma$. La \blue{palabra vacía} que corresponde a $\Sigma^0$ la denotaremos por $\lambda$.


Si $w$ es una palabra sobre el alfabeto $\Sigma$ luego una subpalabra $u$ de $w$ es una palabra $u \in \Sigma^*$ tal que $w = vuz$ para algunas $v, z \in \Sigma^*$. Si $w = vu$ entonces $v$ es un prefijo de $w$ y $u$ es un subfijo de $w$.


\begin{deff}
	El \blue{monoide libre} sobre un alfabeto $\Sigma$ es el siguiente conjunto
	\begin{equation*}
	\Sigma^{*} = \bigcup_{k=0}^{\infty} \Sigma^k
	\end{equation*}
	con la operación $\cdot$ que es la concatenación de palabras. 
	Es decir dadas $w_1 \in \Sigma^{k}, w_2 \in \Sigma^{l}$ luego $w_1 \cdot w_2 \in \Sigma^{k+l} \subset \Sigma^*$. El elemento neutro es la palabra vacía que corresponde a la copia de $\Sigma^0$ que es la única palabra sin letras. 
\end{deff}
\begin{obs}
	El monoide es libre con la siguiente propiedad: si tenemos una función del alfabeto $f: \Sigma \to M$ donde $M$ es algún monoide entonces existe un único morfismo de monoides $\overline f: \Sigma^{*} \to M$ que hace conmutar al siguiente diagrama.	
	
	\begin{center}
	\begin{tikzcd}
		\Sigma \arrow[r, "f"] \arrow[d, hook] & M \\
		\Sigma^* \arrow[ru, "\overline f",dashed,swap]    &  
	\end{tikzcd}
	\end{center}
	
\end{obs}
\begin{deff}
	Un \blue{lenguaje} $L$ sobre un alfabeto $\Sigma$ es un subconjunto de $\Sigma^*$.
\end{deff}




\subsection{Gramáticas.}

\begin{deff}
	Una \blue{gramática} es una tupla ${\cal G} = (V, \Sigma, P, S)$ donde:
	\begin{itemize}
		\item $V$ es un conjunto finito denominado las \emph{variables};
		\item $S \in V$ es el \emph{símbolo inicial};
		\item $\Sigma$ es un conjunto finito disjunto de $V$ que denominamos \emph{símbolos terminales};
		\item $P \subseteq (V \cup \Sigma)^*V(V \cup \Sigma)^* \times (V \cup \Sigma)^*$ es un conjunto finito de \emph{producciones}.
	\end{itemize}
\end{deff}

Dada una gramática ${\cal G} = (V, \Sigma, P, S)$ a cualquiera de sus producciones $(\gamma, \nu) \in P$, la vamos a denotar por medio de la siguiente notación $\gamma \to \nu$. 

A partir de una gramática $\cal G$ podemos definirnos una relación sobre las cadenas $(\Sigma \cup V)^*$. 
Dados $x,y \in (\Sigma \cup V)^*$ diremos que $x$ \emph{deriva} en $y$ si existen $u,v,w,z \in (\Sigma \cup V)^*$ tales que $x = uwv$ y tenemos una producción $w \to z \in P$ de manera que $y=uzv$.
La notación que usaremos es $x \Rightarrow_{\cal G} y$. 
Consideremos la clausura transitiva y reflexiva de esta relación que denotaremos por $\deriva$.

%Dados $uAv \in (\Sigma \cup V)^* V(\Sigma \cup V)^*$ y $w \in (\Sigma \cup V)^*$ si tenemos alguna producción $uAv \to w$ diremos que $uAv$ \emph{deriva} en $w$ y lo denotaremos $uAv \Rightarrow_{\cal G} uwv $. 
%Esto nos define una relación $\Rightarrow$ sobre $(\Sigma \cup V)^*$. 
%Tomamos la clausura transitiva, simétrica y reflexiva para así obtener una relación de equivalencia que denotaremos $\deriva$.
 

\begin{deff}
	Dada una gramática $\cG$ consideramos el \blue{lenguaje generado por la gramática} 
	\[
	L({\cal G}) = \{ w \in \Sigma^* \ | \ S \overset{*}{\Rightarrow_{\cal G}} w   \}.
	\]
\end{deff}

Este lenguaje está formado por las palabras en $\Sigma^*$ que se pueden derivar del símbolo inicial. 

\medskip
\begin{ej}\label{gramatica-regular}
	Consideremos la siguiente gramática ${\cal G} = (V, \Sigma, P, S)$ donde $V = \{ S, A \}, \Sigma = \{ a,b \}$ y tenemos las siguientes producciones,
	\begin{align*}
	S & \to Ab \\
	A & \to aA \\
	A & \to \lambda
	\end{align*}
	Veamos como podemos derivar la palabra $a^2b$ usando las producciones de esta gramática. 
	Esto es que $S \deriva a^2b$.
	Tomamos la siguiente sucesión:
	\begin{align*}
		S \to Ab \to aAb \to aaAb \to aab
	\end{align*} 
	y nos queda tal como queríamos ver.
	
	Más aún probemos que $L({\cal G}) = \{ a^{k}b : k \ge 0 \}$. 
	
	Si $w \in L(\cal G)$ entonces $S \deriva w$ por definición. 
	La única producción que la gramática tiene donde $S$ está a la izquierda es $S \to Ab$. 
	De esta manera cualquier palabra $w \in L(\cal G)$ va a tener una $b$ como postfijo de la palabra. 
	La variable $A$ vemos que solo puede derivar en $a^{k}A$ o $a^{k}$ para $k \ge 0$.
	Esto se debe a que podemos aplicar la producción $A \to aA$ tantas veces como querramos por lo tanto $A \deriva a^k$ para cualquier $k \ge 0$.
	Juntando con lo anterior vemos que $S \deriva a^{k}b$ así que terminamos de ver que la gramática genera al lenguaje $\{a^kb : k \ge 0\}$ tal como queríamos ver.
\end{ej}



Es posible clasificar los lenguajes a partir de las características de las gramáticas que los generan. 

\subsection{Lenguajes regulares.}

\begin{deff}
	Decimos que una gramática $\gramatica$ es \blue{ regular} si las producciones son del estilo
	\begin{enumerate}
		\item $A \to \lambda$
		\item $A \to a$
		\item $A \to a B$
	\end{enumerate}
	donde $A, B \in V$, $a \in \Sigma$ y $\lambda$ es la palabra vacía. 
	Si $L=\lengderivado$ para alguna gramática regular $\cal G$ entonces diremos que $L$ es un \blue{lenguaje regular}. 
\end{deff}

En particular la gramática del ejemplo \ref{gramatica-regular} es regular. 
De esta manera $L= \{ a^k b : \ k \ge 0  \}$ resulta ser un lenguaje regular.


\begin{deff}
	Un \blue{autómata finito no determinístico} es una tupla ${\cal M} = (Q,\{q_0\},\Sigma,\delta,F)$ donde:
	\begin{itemize}
	\item $Q$ es un conjunto finito que denominamos \emph{estados}.
	\item $q_0 \in Q$ es el \emph{estado inicial}.
	\item $\Sigma$ es un conjunto finito que denominamos \emph{alfabeto}.
	\item $\delta:Q \times \Sigma \to {\cal P}(Q)$ es la \emph{función de transición}.
	\item $F \subseteq Q$ es un subconjunto de estados que llamaremos \emph{finales}.
	\end{itemize}
\end{deff}

Un par $(q,w) \in Q \times \Sigma^*$ lo llamaremos una configuración del automáta.
Consideremos que $w = aw'$ donde $a \in \Sigma$ y $w' \in \Sigma^*$ luego si tenemos que $p \in \delta(q,a) $ entonces denotaremos esto así $(q,w) \vdash (p,w')$.
Esto nos define una relación sobre $Q \times \Sigma^*$ tal que si tomamos la clausura transitiva y simétrica obtenemos una relación de equivalencia sobre este conjunto.


\begin{deff}
	El lenguaje generado por un autómata finito no determinístico $M$ es
	
	\[
		L({\cal M} ) = \{  w \in \Sigma^*, q_F \in F \mid (q_1,w) \vdash^* (q_F,\lambda)     \}
	\]
	
	
\end{deff}

\begin{obs}
	Al ser un autómata finito no determinístico existen posiblemente más de una manera de consumir alguna palabra en el autómata. 
	Es así que por la definición que dimos la palabra es aceptada por el autómata si al menos alguna de estas derivaciones la lleva a ser aceptada.
	No importa si existen algunas que no lo hagan mientras una sí lo haga. 
\end{obs}

%Como vimos en este ejemplo el lenguaje $L = \{ a^kb \ : \ k \ge 0  \}$ es aceptado por un automáta no determinístico finito y por lo que sabíamos del ejemplo \ref{gramatica-regular} es un lenguaje regular.
Vale que los lenguajes aceptados por automátas finitos no determinísticos son justamente los regulares.

\begin{teo}
 Un lenguaje $L$ es regular sii es aceptado por un autómata finito no determínistico.
\end{teo}

\begin{proof}
	Demostración estándar. Ver \cite{hopcraft-ullman}.
\end{proof}




%definición automáta finito
%definición de lenguaje regular por medio del automáta, las otras defs no son necesarias
% comentar que los regulares son cerrados para grupos? quizá pueda servir para después





\subsection{Lenguajes independientes de contexto.}
\begin{deff}
	Una gramática $\gramatica $ es \blue{independiente de contexto} si las producciones tienen la siguiente forma:
	\begin{equation*}
	A \to w
	\end{equation*}
	donde $A \in V, w \in (\Sigma \cup V)^*$.  
	Si $L=\lengderivado$ para alguna gramática independiente de contexto $\cal G$ entonces diremos que $L$ es un \blue{lenguaje independiente de contexto}.
\end{deff}

\begin{obs}
	Todo lenguaje regular en particular resulta ser un lenguaje \ic.
\end{obs}


\begin{ej}\label{leng_ej_gram_palindromos}
	Sea el alfabeto $\Sigma = \{ a,b \}$. Si $w=a_1 \dots a_k$ es una palabra sobre $\Sigma^*$ entonces podemos considerar a $w^r$ que es la palabra inversa dada por leerla de derecha a izquierda y tiene la siguiente pinta $w^r= a_k \dots a_1$. 
	Consideremos sobre $\Sigma$ el lenguaje 
	\[
	L = \{ w \in \Sigma^* \ | \ w = w^r  \}
	\]
	 tal que este es el lenguaje de los palíndromos. 
	Construyamos una gramática independiente de contexto para este lenguaje.
	Sea $w$ una palabra en $L$ luego sabemos que si su longitud es mayor que uno entonces debe ser que $w = a u a$ o $w = b u b$ para cierta palabra $u \in L$ dado que $u$ necesariamente tiene que ser un palíndromo porque $w$ lo es. 
	Esto sucede para todas las palabras del lenguaje exceptuando las palabras de longitud uno que son justamente las letras del alfabeto. 
	De esta manera podemos considerar la siguiente gramática ${\cal G}  =  (\{S\}, \Sigma, P, S )$ donde las producciones $P$ están dadas por :
	\begin{align*}
	S  & \to \epsilon \\ S &\to a \\ S &\to b \\ S &\to  aSa \\ S &\to bSb. \\
	\end{align*}
	Para ver que esta gramática genera al lenguaje $L$ notemos que si tomamos una palabra en el lenguaje $w \in L$ luego si no es una letra al ser palíndromo sobre el alfabeto $\Sigma$ necesariamente debe comenzar con $a$ o con $b$ y de esta manera tenemos que la primer derivación es $S \to aSa$ o $S \to bSb$. 
	Dado que la subpalabra $w = aua$ que se obtiene de $w$ sin considerar la primera y última letra es un palíndromo (e incluso podría ser la palabra vacía) luego podemos repetir este proceso para llegar a $S \deriva w$ después de finitos pasos. 
	Por otro lado toda palabra generada por esta gramática es un palíndromo porque todas las reglas son tales que agregan una letra al principio y la misma al final o simplemente agregan letras que también son palíndromos.
\end{ej}

Toda gramática independiente de contexto la podemos tomar para que sea de una forma en particular.

\begin{deff}
	Una gramática $\gramatica$ independiente de contexto está en su \blue{forma normal de Chomsky} si las producciones son de este tipo:
	\begin{enumerate}
		\item[\textbf{CH1.}] $A \to BC$ donde $A\in V$ y $B,C \in V \setminus \{ S \}$.
		\item[\textbf{CH2.}] $A \to a$ donde $A \in V, a \in \Sigma$.
		\item[\textbf{CH3.}] $S \to \lambda$ 
	\end{enumerate}
\end{deff}



\begin{prop}
	Para toda gramática $\cal G$ independiente de contexto puede tomar otra $\cal G'$ tal que esté en forma normal de Chomsky y $\lengderivado = L(\cal G')$,
\end{prop}

\begin{proof}
	Demostración estándar. Ver \cite{hopcraft-ullman}.
\end{proof}

\subsection{Autómatas de pila.}

\begin{deff}
	Un \blue{autómata de pila finito no determinístico} es una tupla 
	\[
	{\cal M } = (Q, \Sigma, Z, \delta, q_0, F, Z_0)
	\]
	 donde:
	\begin{itemize}
		\item $Q$ es un conjunto finito denominado los \emph{estados};
		\item $\Sigma$ es un conjunto finito que denominamos el \emph{alfabeto del lenguaje};
		\item $\Gamma$ es un conjunto finito que denominamos el \emph{alfabeto de la pila};
		\item $\delta$ es la \emph{función de transición} donde $\delta: Q  \times \Sigma \cup \{ \lambda \} \times \Gamma \to {\cal P}( Q  \times \Gamma^*)$;
		\item $F \subseteq Q$ es el conjunto de \emph{estados finales};
		\item $q_0 \in Q$ es el \emph{estado inicial};
		\item $Z_0 \in Z$ es el \emph{símbolo inicial} en la pila.
	\end{itemize}
\end{deff}

\begin{obs}
	Bajo esta definición de un \APND notemos que no hay transición posible si en el tope de la pila está el símbolo inicial de la pila. 
\end{obs}


Una configuración de un autómata de pila no determinístico va a ser un triple $(q,w,\gamma) \in Q \times \Sigma^* \times Z^*$.
Sea $w = aw'$ con $a \in \Sigma$ y $w' \in \Sigma^*$ tal que $(p,\gamma') \in \delta (q,a,\gamma)$ luego denotaremos $(q,w,\gamma) \vdash_{\cal M} (p,w',\gamma')$ la transición de una configuración en otra en el \APND $\cal M$.
Esta operación nos define una relación sobre $Q \times \Sigma^* \times Z^*$.
Consideremos la relación de equivalencia generada por la clausura transitiva y simétrica de esta operación.
Así como en el caso de los autómatas finitos la denotaremos $\vdash^*_{\cal M}$.


\begin{deff}
	Dado $\cal M$ un autómata de pila no determinístico consideramos el lenguaje generado por estado final como
	\begin{equation*}
		L( {\cal M}) = \{ w \in \Sigma^* \ | \ (q_0,w,Z_0) \vdash^* (q, \lambda, \gamma), \ \ q \in F, \ \gamma \in \Gamma^*      \}
	\end{equation*}
\end{deff}


%\paragraph{Descripción instantánea del autómata.} Veamos ahora como describir formalmente el funcionamiento de un autómata a partir de lo que estamos haciendo en cierto instante. 
%Consideremos que estamos en el instante que nuestra subpalabra que nos queda por leer es $w$, estamos en un estado $q$ y en nuestra pila tenemos la palabra $\gamma$, entonces vamos a representar al instante por medio de esta tupla $(q,w,\gamma)$.
%Si ahora tenemos la posibilidad de movernos a otro estado $p$ tal que $(p,w',\alpha\beta) \in \delta (q,aw',x\beta)$ donde $aw' = w$ con $a$ alguna letra posiblemente vacía y similarmente $x \beta = \gamma$ con $x \in \Gamma^*$ una letra de $\Gamma$ posiblemente vacía. 
%Este movimiento lo denotamos como $(q,aw',x\beta) \vdash (p,w',\alpha \beta)$. 
%Podemos considerar la clausura transitiva de esta relación sobre los triples $Z \times Q \times \Sigma^*$ que denotaremos $\vdash^*$.


%\paragraph{Funcionamiento del autómata.}
%Un prefijo de $w$ será una subpalabra $\gamma$ tal que $w=\gamma w'$ visto con la concatenación de palabras en $\Sigma^*$.
%
%El autómata de pila finito funciona de la siguiente manera. 
%Dada una palabra $w \in \Sigma$ queremos saber si es aceptada por el autómata de pila o no. Para eso vamos a ir leyendo esta palabra de izquierda a derecha. 
%Al comenzar a leer esta palabra estamos en el estado $q_0$ que distinguimos como el estado inicial de nuestro autómata. 
%Nos fijamos en la función de transición que es una función parcial cuánto nos da evaluada  $\delta(\lambda,q_0,\gamma)$ para algún $\gamma$ prefijo de $w$ y donde estamos mirando a $\lambda \in Z^*$ el elemento neutro de este monoide. 
%Esto se corresponde a la idea de que al comenzar nuestra pila está vacía. 
%En tal caso nuestra función de transición nos da un resultado que es un par $(z,q)$ donde $z \in Z^{*}$ es lo que nos va a quedar en la pila y $q$ es el nuevo estado al cual nos movimos. 
%Notemos que nuestra función de transición no tiene porqué tener un $(z,q)$ tal que podamos movernos o podría ser bien que tenga más de uno. 
%En este caso diremos que nuestro autómata es \blue{no determinístico} dado que en algunos casos existe más de una opción.
%
%En general estamos en la siguiente situación en el proceso de aceptar la palabra $w$. 
%Tenemos algún $z \in Z$ en el tope de la pila que al ser una pila lo leeremos de derecha a izquierda, estamos en algún estado $q \in Q$ y nos quedará una subpalabra $\gamma$ de $w$ para leer. Una \blue{configuración} de nuestro autómata entonces es una manera de describir en que situación de aceptar o no aceptar una palabra y la denotamos $(z,q,\gamma)$. 
%
%Cuando hayamos visto toda la palabra o no tengamos manera de movernos de estado nos fijamos si el estado $p$ en el que estamos es final, es decir si $p \in F$. 
%En tal caso la palabra $w$ es aceptada por el autómata. Formalmente estaremos en alguna configuración $zq$ para $z \in Z^*$ y $q \in Q$ y no nos queda nada de la palabra $w$ porque ya la consumimos toda.
%\medskip
%\begin{obs}
%	Así como definimos el \APND para que al tener la pila vacía se detenga podríamos haberlo hecho de una manera distinta por ejemplo dejando que la pila sea vacía y así poder transicionar de una configuración a otra.	
%	Esta manera es equivalente porque...
%	Más en adelante nos va a resultar conceptualmente más útil para el caso que estemos viendo el problema de la palabra de un grupo mientras que para las demostraciones de esta sección usaremos esta otra definición.
%\end{obs}



%
%\paragraph{El lenguaje aceptado por un autómata de pila.} 
%Notemos que en particular el autómata de pila nos da un lenguaje que está formado por las palabras $w$ en el alfabeto de la entrada del autómata $\Sigma$ que son aceptadas. 
%En general diremos que un autómata acepta un lenguaje $L$ si su lenguaje aceptado es exactamente $L$. 
%Este lenguaje aceptado por el autómata $\cal M$ en algunas casos para hacer énfasis en el autómata 


\begin{ej}
	Sea nuestro alfabeto $\Sigma = \{ a, b\}$ y $L$ el lenguaje de los palíndromos sobre este alfabeto tal como lo definimos en el ejemplo \ref{leng_ej_gram_palindromos}.
	Consideremos el siguiente autómata de pila 
	\[
	M=(Q,\{a,b\}, \{a,b,\$\} , \delta, q_0, \{q_1\}, \$)
	\]
	donde nuestra pila tiene el mismo alfabeto que el de entrada con un símbolo adicional que va funcionar como nuestro símbolo inicial de la pila. 
	El autómata tiene dos estados, el inicial y el final. 
	Al automáta de pila lo representaremos de la siguiente manera:
	\begin{center}
		\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3.5cm,
		scale = 1,transform shape]
		
		\node[state,initial] (q_0) {$q_0$};
		\node[state,accepting] (q_1) [right of=q_0] {$q_1$};
		
		\path (q_0) edge [bend right]             node {} (q_1);
		\path (q_1) edge [loop above] node {}    (   );
		\path (q_0) edge [loop above] node {}    (   );
		\end{tikzpicture}
	\end{center}
	
	La idea es que en el primer estado vamos apilando la palabra y en la segunda vamos desapilando la palabra anteriormente apilada. 
	Nuestra función de transición $\delta$ va a ser la siguiente,
	\begin{itemize}
		\item $\delta(q_0,a,Z) = (q_0,aZ)$ 
		\item $\delta(q_0,b,Z) = (q_0,ba)$ 
		\item $\delta(q_1,a,a) = (q_0,\lambda)$.
		\item $\delta(q_1,b,b) = (q_0,\lambda)$.
		\item $\delta (q_0, \lambda, Z) = (q_1,Z)$
		\item $\delta (q_0, X, Z) = (q_1, Z)$ 
	\end{itemize}
	donde $Z$ es decir cualquier elemento del alfabeto de la pila y $X$ cualquier elemento de nuestro alfabeto de entrada. 
	El autómata en el primer estado apila lo que sea que estemos leyendo sin importar lo que esté en el tope de pila. 
	En el segundo estado desapila cada vez que lo que estemos leyendo coincida con el tope de pila. 
	Finalmente para ir del estado inicial al final tenemos en cuenta dos casos. 
	Nos podemos mover por $\lambda$ es decir sin consumir ninguna letra de la palabra de la entrada o leyendo alguna de las letras de la palabra. 
	Estos casos se corresponden a que el palíndromo tenga longitud par o tenga longitud impar.	
\end{ej}

Este ejemplo nos da un indicio que los lenguajes aceptados por autómatas de pila también podrían ser \ic y esto efectivamente es cierto.

\medskip

\begin{teo}\label{teo_ic_apnd}
Un lenguaje $L$ es independiente de contexto sii es aceptado por un autómata de pila no determinístico.
\end{teo}

\begin{proof}
	%Demo importante diría si bien es super estándar y conocidísimo el resultado.
	Ver \cite{hopcraft-ullman}.
\end{proof}
 


Hasta ahora definimos los autómatas de pila no determinísticos que aceptan por estado final. Otra definición posible de lenguaje aceptado podría ser que acepten por pila vacía. 
Es decir que una vez que consumimos la palabra $w$ de entrada llegamos a una configuración $(q, \lambda, \lambda)$ donde $\lambda$ es la palabra vacía de ambos alfabetos respectivamente. Formalmente dado un autómata de pila $\cal M$  notaremos al lenguaje aceptado por pila vacía de la siguiente manera,
\begin{equation*}
	N({\cal M}) = \{ w \in \Sigma^* \ | \ (q_0,w,Z_0) \vdash^* (q, \lambda, \lambda), \ q \in Q, u \in \Sigma^*    \}
\end{equation*}
donde arrancamos en el estado inicial y llegamos a algún estado $q$ cualesquiera sin importar que sea final o no.
La pila está vacía así como lo que nos queda por leer de la palabra. 
En este caso diremos que nuestro lenguaje es aceptado por un autómata de pila no determinístico por pila vacía.


El siguiente resultado nos dice que en el caso que nuestro autómata sea no determinístico es equivalente usar una u otra manera de definir a nuestro lenguaje.

\medskip
\begin{teo}
Un lenguaje $L$ es aceptado por un autómata de pila no determinístico por estado final si y solo sí es aceptado por un autómata de pila no determinístico por pila vacía.
\end{teo}

\begin{proof}
	%Demo bastante fácil si se hacen los dibujitos. Quizá ni haga falta escribirla.
	Ver \cite{hopcraft-ullman}.
\end{proof}


\subsection{Propiedades de los lenguajes independientes de contexto.}

En esta sección vamos a considerar a los lenguajes \ic como una clase. 
Probemos algunas propiedades que cumplen respecto a las operaciones de conjuntos más usuales.

\begin{prop}\label{intersecciones-reg-ic}
	Los lenguajes \ic son cerrados por intersecciones con lenguajes regulares.
\end{prop}

\begin{proof}
	Sea $L$ un lenguaje \ic tal que $L \subset \Sigma^*$ y sea $R$ un lenguaje regular tal que $R \subset \Sigma^*$. 
	Queremos ver que el lenguaje $L \cap R$ es aceptado por un \APND.
	
	Si $L$ es aceptado por un \APND ${\cal M } = (Q, \Sigma, Z, \delta, q_0, F, Z_0)$ y R es aceptado por un autómata no determinístico ${\cal M' } = (Q', \Sigma, \delta', q_0', F',)$.
	
	Consideremos el siguiente \APND
	\[
		{\cal N } = (Q \times Q', \Sigma, Z, \delta \times \delta', (q_0,q_0'), F \times F', Z_0).
	\]	
	
	Queremos ver que $L({\cal N}) = L \cap R $.	
	Probemos primero la siguiente afirmación.
	\[
	((q_0,q_0'), w, Z_0) \vdash^*_{\cal N}  ((q_i,q_j), \lambda, z) \ \iff (q_0, w, Z_0) \vdash^*_{\cal M}  (q_i, \lambda, z) \ \text{y} \ (q_0', w) \vdash^*_{\cal M'} (q_j, \lambda)  	
	\]
	Para ver esto veremos las dos implicaciones a la vez haciendo inducción en la longitud de la palabra $w$.
	
	En el caso base $|w| = 1$ de manera que $w = a \in \Sigma$.
	Este caso tenemos la igualdad porque justamente la función de transición del \APND $\cal N$ es $\delta \times \delta'$.
	
	Para el paso inductivo consideremos que $|w|=n$ de manera que $w=ua$ con $|u|=n-1$ y $a \in \Sigma$.
	Luego tenemos que $((q_0,q_0'), u, Z_0) \vdash^*_{\cal N}  ((q_k,q_l), \lambda, z') \ \iff (q_0, u, Z_0) \vdash^*_{\cal M}  (q_k, \lambda, z') \ \text{y} \ (q_0', u) \vdash^*_{\cal M'} (q_l, \lambda)$.
	Nuevamente usamos nuestra definición de la función de transición $\delta \times \delta'$ para concluir que
	\[ 
	((q_i,q_j),\lambda, z) \in \delta \times \delta'((q_k,q_l), a, z') \iff  (q_i, a, z') \in \delta(q_k, a, z') \ \text{y} \ (q_j, a) \in \delta(q_l, a).
	\]	
	Con esto terminamos de probar la afirmación.
	
	Finalmente para ver que $L({\cal N})  =  L \cap R$ usamos que por nuestra afirmación si $w \in L \cap R$ entonces es aceptado por lo dos autómatas $\cal M$ y por $\cal M'$, esto es que $(q_0, w, Z_0) \vdash^* (q_F, \lambda, z)$ y que $(q_0', w) \vdash^* (q_F', \lambda)$ y equivalentemente $((q_0,q_0'),w,Z_0) \vdash^* ((q_F, q_F'),\lambda, z)$. 
	Dado que los estados finales del autómata $\cal N$ resultan ser $F \times F'$ obtenemos que $w \in L({\cal N}) \iff w \in L \cap R$.
	Concluimos así que el lenguaje $L \cap R$ resulta ser \ic tal como queríamos ver.		
\end{proof}

Otras propiedades interesantes tienen que ver con su relación con morfismos de monoides. 

\begin{prop}\label{morfismos-monoides-ic}
	Los lenguajes \ic son cerrados por:
	\begin{enumerate}
		\item Imágenes de morfismos de monoides.
		\item Preimágenes de morfismos de monoides.
	\end{enumerate}
\end{prop}

\begin{proof}
	\begin{enumerate}
		\item Consideramos $L$ lenguaje \ic sobre $\Sigma$.
		Esto nos dice que existe una gramática $\gramatica $ tal que $L(\cG) = L$.
		Si tenemos un morfismo de monoides $h: \Sigma^* \to \Delta^*$ queremos ver que existe una gramática $\cG'$ tal que $L(\cG') = h(L)$.
		Para eso consideramos $\cG  = (V, \Sigma, P', S)$ donde la única diferencia es que reemplazamos en cada producción $P \in \cG$ a cada letra $a \in \Sigma$ por $h(a) \in \Delta^*$.
		La gramática sigue siendo \ic, nos alcanza con ver que genera al lenguaje que queremos.
		
		Miramos las dos contenciones a la vez.		
		Si tomamos una palabra $w \in L$ luego tenemos que $h(w) \in h(L)$ por lo tanto tiene que haber una sucesión de producciones $S  \implies^* w  $ en la gramática $\cG$.
		Si tomamos esta derivación y cambiamos cada producción $A \to \alpha \in P$ por la correspondiente $A \to \beta \in P'$, donde $\beta = h(\alpha)$, en la gramática $\cG'$ ahora obtenemos la palabra $h(w)$.
		Volviendo para atrás obtenemos que $L(\cG') = h(L)$ tal como queríamos ver.
		
		\item Sea $L$ un lenguaje \ic sobre el alfabeto $\Sigma$ tal que es aceptado por un \APND ${\cal M } = (Q, \Sigma, Z, \delta, q_0, F, Z_0)$.
		Consideremos un morfismo de monoides $h: \Delta^* \to \Sigma^*$. 
		Queremos ver que el lenguaje $h^{-1}(L) \subset \Delta^*$ resulta ser \ic. 
		Primero introduzcamos la constante $ n = \max_{a \in \Delta} |h(a)|$.
		
		Ahora consideremos el siguiente \APND 
		\[
			{\cal M' } = (Q', \Delta, Z, \delta, q_0', F', Z_0)
		\]
		donde $Q' = Q \times \Sigma^{\le n}$, donde $\Sigma^{\le n} = \{ w \in \Sigma^* : |w| \le n  \}$.
		Los estados finales $F' = F \times \{ \lambda \}$.
		El estado inicial es $q_0' = (q_0, \lambda)$.
		Finalmente nuestra función de transición resulta ser 
		\[
		\delta'((q,v), a ,z) = 
		\begin{cases}
		\{(q,h(a), z )\}  \ &\text{si} \ v=\lambda, z \in Z \\
		\{(p,u),z' \} \ &\text{si} \ (p,z') \in \delta(q,y,z), v=yu \\  
		\emptyset \ &\text{caso contrario}.
		\end{cases}
		\]
		
		Probemos primero la siguiente afirmación.
		Para cada $w \in \Delta^*$ tenemos la siguiente equivalencia,
		\[
			(q_0',w,z) \vdash^*_{\cal M'} ((q,\lambda), \sigma) \iff (q_0,h(w), z) \vdash^*_{\cal M} (q, \sigma).
		\]
		
		Para probar esta equivalencia lo hacemos por inducción en la longitud de $|w|$.
		Para el caso base tenemos que $w = a \in \Delta$.
		En este caso tenemos que $(q_0', a, z) \vdash ((q,h(a)), z)$.
		Una vez en este estado por como definimos la función de transición tenemos que $ (q_0,h(a), z) \vdash (q, \sigma)  \iff ((q_0,h(a)), \lambda ,z) \vdash ((q,\lambda), \sigma)$.
		
		El caso general tenemos una palabra $w$ tal que $|w|=n$.
		Sabemos que la afirmación vale para cualquier palabra de longitud menor a $n$.
		En particular si $w=ua$ con $|u|=n-1$ y $a \in \Delta$, tenemos que por la hipótesis inductiva que 
		\[
		(q_0',u,z) \vdash^*_{\cal M'} ((q,\lambda), \sigma) \iff (q_0,h(u), z) \vdash^*_{\cal M} (q, \sigma)
		\]
		entonces de nuevo por el mismo razonamiento que hicimos para el caso base tenemos probada la afirmación.
		
		Para concluir la demostración notemos que por nuestra afirmación una palabra $w \in L(\cal M')$ si y solo si $h(w) \in L$. 
		Esto nos dice que el lenguaje aceptado por estado final de $\cal M'$ resulta ser $L' = \{ w \in \Delta^* : h(w) \in L \} = h^{-1}(L)$.
	\end{enumerate}
\end{proof}

Finalmente la herramienta principal que tenemos para ver que cierto lenguaje $L$ no es \ic es usar el siguiente lema.

\begin{lema}[Pumping] \label{pumping}
	Sea $L$ un lenguaje independiente de contexto entonces existe una constante $n \ge 0$ tal que para todas las palabras $w \in L$ de longitud al menos $n$ existe una factorización $w = uvxwy$ con $|vwx| \le n$ y $|vx| > 0$ tal que para todo $i \in \NN$ vale que $uv^iwx^iy \in L$.
\end{lema}

\begin{proof}
	Resultado estándar. Ver \cite{hopcraft-ullman}.
\end{proof}

\subsection{Autómatas de pila determinísticos.} 
Si en la definición anterior del autómata pedimos que la función de transición de una configuración dada tenga a lo sumo un valor entonces nuestro autómata lo vamos a llamar determinístico. 
Formalmente esto es que 
\[
|\delta(q,a, z)| \le 1 \ \ \ \forall z \in Z, \ p \in Q, \ a \in \Sigma \cup \{ \lambda \}.
\]
En cierta manera estamos diciendo que de un instante dado solo tenemos a lo sumo una única posibilidad de movernos a otro estado. 
A los lenguajes aceptados por un \APD los pensamos que son aceptados por estado final.


Por lo tanto para cada palabra tenemos un único camino en el autómata para saber si es aceptada o no a diferencia de un \APND  que podría tener varios caminos posibles para cada palabra.
\begin{obs}
	Todo \APD en particular es no determinístico y por lo tanto la clase de lenguajes aceptados por los primeros están contenidos en la clase de los segundos.
\end{obs}

Veamos que esta contención es estricta. Para eso volvamos a considerar el ejemplo del lenguaje de los palíndromos.
\begin{ej}
	El lenguaje 
	\[
	L = \{ w \in \{ a,b \}^*  \ : \ w = w^r \}
	\]
	 no es aceptado por un \APD pero sí por uno no determinístico. 
	Supongamos que $M$ es un \APD que lo acepta. 
	
	Notemos que para cualquier palabra $w \in \{ a,b \}^*$ debe ser que al consumirla la pila no puede quedar vacía dado que $ww^r \in L$ y en tal caso no aceptaría a esta palabra. 
	Esto es que la pila nunca está vacía sea cual sea la configuración que lleguemos. 
	Para cada palabra arbitraria $w$ existe otra $x_w$ tal que al procesar $wx_w$ lo que nos queda en la pila es de tamaño mínimo con respecto a todas las palabras $wx$. 
	Sea entonces lo que tiene en la pila la palabra $\alpha_w$ que sabemos es de longitud mínima. Si consideramos palabras del estilo $wx_wz$ sabemos que la longitud de lo que quede en la pila no puede disminuir. 
	Ahora consideremos dos palabras del estilo 
	
	\[
	r=wx_w, \ s=uy_u
	\]
	 tales que sus pilas son de longitud mínima al terminar de recorrer las palabras y que resultan tener el mismo tope de pila y terminar en el mismo estado. 
	Podemos asegurar la existencia de estas palabras debido a que tenemos finitos estados y combinaciones de tope de pilas dado que el autómata de pila es finito pero tenemos infinitas palabras que cumplen esta propiedad. 
	Ahora basta con elegir $z$ de modo que $tz$ sea palíndromo pero que $sz$ no lo sea. 
	
	Veamos que podemos elegir a $z$ para que una de las concatenaciones $tz,sz$ sea palíndromo y la otra no. Partamos en distintos casos. 
	
	Si $|t|=|s|$ basta con tomar $z=s^r$. 
	Si $|t|\neq |s|$ y supongamos que $s$ tiene longitud menor y no es prefijo de $t$ entonces de nuevo podemos tomar el palíndromo $ss^r$ tal que $ts^r$ no es un palíndromo.
	 
	Finalmente queda el caso que una es un prefijo de la otra, supongamos $t=su$. 
	Si elegimos $x=a,b$ tal que $ux$ no sea un palíndromo luego la palabra $ss^r$ es un palíndromo pero $suxs^r$ no lo es.
	
	Esto muestra que si bien $sz \notin L$ y $tz \in L$ el \APD no va a poder diferenciarlas por lo tanto no es posible que este lenguaje sea aceptado por un \APD tal como queríamos ver.
	
\end{ej}

\subsection{Autómatas de pila determinísticos especiales.} 
Consideremos ahora un \APD tal que acepta tanto por estado final como por pila vacía. A estos los llamaremos \textit{autómatas de pila determinístico especiales}.  
Estos autómatas son los que nos surgen de la construcción del autómata del problema de la palabra  para grupos virtualmente libres. 

\begin{ej}
	Sea el lenguaje $L = \{ a^m b^n  : m \ge n \ge 1 \}$ este no es un lenguaje independiente de contexto determinístico especial pero sí es determinístico. 
	
	Construyamos un \APD que acepte a $L$. Sea $$M = (\{q_0,q_1,q_2\}, \{q_0\}, \{a,b\}, \{a,b,Z_0\}, Z_0, q_2) $$ el siguiente \APD que representamos así:
	
	\begin{center}
		\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3.5cm,
		scale = 1,transform shape]
		
		\node[state,initial] (q_0) {$q_0$};
		\node[state] (q_1) [right of=q_0] {$q_1$};
		\node[state,accepting] (q_2) [right of=q_1] {$q_2$};
		
		\path (q_0) edge    [bend right]          node {$b, a | \lambda$} (q_1)
		(q_0) edge    [loop above]          node {$a, Z | aZ$} (q_0)
		(q_1) edge      [bend right]      node {$\lambda, a | Z$}   (q_2)
		(q_1) edge    [loop above]           node {$b, a | \lambda$} (q_1);
		
		\end{tikzpicture}
	\end{center}
	
	
	
	donde $Z$ es cualquier letra del alfabeto de la pila. El autómata en el estado inicial $q_0$ apila a todas las $a$ y cambia al estado $q_1$ cuando lee por primera vez una $b$ y en ese caso desapila la a que está en el tope de la pila. En el estado $q_1$ sigue desapilando cada vez que ve una $b$. Finalmente va al estado $q_2$ cuando en la pila sigue quedando $a$ y ya leímos toda la palabra y en tal caso la acepta porque significa que vimos como máximo tantas $b$ como $a$ y este es el lenguaje que buscábamos generar.
	
	El lenguaje no es aceptado por un \APD por pila vacía dado que tiene la propiedad de los prefijos. 
	Es decir que existen palabras que están en el lenguaje tales que alguno de sus prefijos también están. 
	Por ejemplo consideremos
	 \[
	 	a^m b^i, \ a^m b^j \ \
	 	\text{para}  \
	 	m \ge 2,  \ i < j \le m
	 \] 
	Esto es porque si $M = (Q, \Sigma, Z, Z_0, \delta, q_0 , F)$ fuera un \APD que acepta por pila vacía a este lenguaje tendríamos que 
	\[
		(q_0,a^{m}b^{i},Z_0) \vdash^* (q,\lambda, \lambda)
	\]
	con $q$ un estado final pero como tiene la pila vacía no podemos continuar aceptando a la palabra $a^mb^j$ ya que por la definición que empleamos el autómata necesita leer algún elemento de la pila. 
	
	De esta manera vemos que este lenguaje no puede ser aceptado por pila vacía y estado final por un \APD concluyendo que los lenguajes determinísticos especiales forman un subfamilia propia de los independientes de contexto determinísticos.
	
%[Esto es con la otra definición de automáta de pila.]
%			Por el principio del palomar podemos ver que este lenguaje no es aceptado por \APD por pila vacía y estado final a la vez. Si así lo fuera supongamos que existe un automáta que lo acepta $M = (Q, \Sigma, Z, Z_0, \delta, q_0 , F)$. Debido a que tiene finitos estados podemos elegirnos $m$ suficientemente grande tal que existan palabras $a^mb^j, a^mb^i$ con $1 \le i < j < m$ y elegidas de manera que terminen en el mismo estado final $p$. Notemos que $(q_0,a^mb^ib^{m-j+1},Z_0) \vdash^* (q,\lambda, Z)$ donde $q$ es un estado final dado que $m-j+1+i \le m$. Por otro lado debe ser que $(q_0, a^mb^jb^{m-j+1}) \vdash^* (q,\lambda, Z) $ pero notemos que $a^mb^{m-1} \notin L$. Por lo tanto llegamos a una contradicción y de esta manera vemos que $L$ no es aceptado por un \APD especial tal como queríamos ver.
\end{ej}


\subsection{El problema de la palabra.}

Todo grupo $G$ lo podemos ver como un monoide. 
En particular si tenemos un grupo $G$ tal que es finitamente generado como grupo por algún conjunto finito $X$ entonces es finitamente generado como monoide por el conjunto simétrico de generadores $Y = X \cup X^{-1}$.
Sea entonces $\Sigma$ conjunto finito de generadores como monoide de $G$
por lo tanto tenemos un epimorfismo de monoides desde el monoide libre $\pi: \Sigma^* \twoheadrightarrow  G$. 

El problema de la palabra es uno de los problemas de teoría de grupos más centrales al área. Explícitamente el problema consiste en encontrar un algoritmo que dada una palabra $\omega \in \Sigma^*$ decida si esta palabra es la identidad del grupo o no. 

\begin{deff}
El \blue{problema de la palabra de $G$ para los generadores $\Sigma$} es el siguiente lenguaje,	
\[ \text{WP} (G, \Sigma) = \{ \omega \in \Sigma^* \ | \ \omega \underset{G}= 1 \}\]
\end{deff}

Dado que este lenguaje depende del conjunto de generadores elegido nos gustaría ver qué condiciones tienen que cumplir los lenguajes o los grupos para que no exista esta dependencia en los generadores.


En el contexto del problema de la palabra nos van a interesar las familias de lenguajes que cumplen las siguientes propiedades.
\medskip
\begin{deff}
	Una familia de lenguajes $\CC$ es un \blue{cono} si para todo $L \in \CC$ resulta que:
	\begin{itemize}
		\item[\textbf{C1.}] Es cerrado por imágenes de morfismos de monoides. Sea $L \subset \Sigma^*$ luego si existe $\phi:\Sigma^* \to \Delta^*$ morfismo de monoides debe ser que $\phi(L) \in \CC$.
		\item[\textbf{C2.}] Es cerrado por preimagenes de morfismos de monoides. Sea $L \subset \Sigma^*$ luego si existe $\phi:\Sigma^* \to \Delta^*$ morfismo de monoides  debe ser que $\phi^{-1}(L) \in \CC$. 
		\item[\textbf{C3.}] Es cerrado por intersecciones con lenguajes regulares. Si $R$ es un lenguaje regular sobre $\Sigma^*$ entonces $L \cap R \in \CC$ también resulta serlo.
	\end{itemize}
\end{deff} 


Los conos de lenguajes cumplen la siguiente propiedad de gran importancia para el estudio del problema de la palabra.
\medskip
\begin{prop}\label{prop-cono-wp}
	Sea $WP(G, \Sigma)$ el lenguaje del problema de la palabra de cierto grupo $G$ para algunos generadores $\Sigma$ y $\CC$ cono de lenguajes. 
	Si $WP(G, \Sigma) \in \CC$ luego valen las siguientes afirmaciones:
	\begin{itemize}
		\item[\textbf{W1.}] $WP(G, \Delta) \in \CC$ para cualquier conjunto de generadores $\Delta$.
		\item[\textbf{W2.}] $WP(H) \in \CC$ para todo subgrupo $H$ \fg de $G$.
	\end{itemize} 
\end{prop}
\begin{proof}
		Para ver \textbf{W1} formamos el siguiente diagrama conmutativo,
		\begin{center}
			\begin{tikzcd}
				\Delta^* \arrow[r, "\delta"] \arrow[d,"f",swap] & G \\
				\Sigma^* \arrow[ru, "\pi",swap]    &  
			\end{tikzcd}
		\end{center}
		donde $f$ es algún morfismo de monoides y donde usamos la propiedad universal de los monoides libres.
		Notemos que $WP(G, \Delta) = \delta^{-1}(1)$ y como el diagrama conmuta tenemos que 
		\[
		f^{-1}(\delta^{-1}(1)) = f^{-1}(WP(G,\Sigma)) = WP(G, \Delta).
		\]
		Dado que esto es un cono obtenemos lo que queríamos ver puesto que es cerrado por preimágenes de morfismos de monoides.
		
		
		
		Veamos ahora que vale \textbf{W2}. 
		Sea $\Sigma'$ conjunto de generadores de $H$.
		Siempre podemos extenderlo a $\Sigma$ tal que $\Sigma$ genere a $G$. 
		De esta manera 
		\[
		WP(H, \Sigma') = WP(H, \Sigma) \cap \Sigma'^*
		\]
		y de vuelta como es un cono la intersección con lenguajes regulares nos da un lenguaje en el cono. 

\end{proof}


Esto nos dice que es interesante estudiar el problema de la palabra justamente sobre conos.


\begin{ej}\label{ic-cono}
	Los lenguajes independientes de contexto forman un cono.
	Esto se puede ver a partir de las proposiciones  \ref{intersecciones-reg-ic} y \ref{morfismos-monoides-ic}.
\end{ej}

Esto nos dice que la siguiente definición no depende de los generadores que tomemos.

\begin{deff}
	Si $G$ es un grupo \fg tal que para ciertos generadores $\Sigma$ resulta que $WP(G, \Sigma)$ es independiente de contexto entonces diremos que $G$ es un \blue{grupo \ic }.
\end{deff}


\begin{ej} Consideremos los siguientes ejemplos.	
	\begin{enumerate}[E1.]
		\item 
		Dado $F$ grupo libre de rango finito supongamos generado por el conjunto finito $X$. 
		Sea  $Y = X \cup X^{-1}$ el conjunto de generadores simétrico de $X$. 
		Probemos que $\text{WP}(F,Y)$ es un lenguaje independiente de contexto.
		
		Consideremos el siguiente autómata de pila,
		\[
		M = (\{ 1 \}, Y, Y, \delta, 1, \{1\}).
		\]
		
		Por como lo definimos tiene un solo estado y el alfabeto tanto de entrada como el de la pila que usa es el conjunto de generadores $Y$.
		La función de transición $\delta$ está definida de la siguiente manera,
		\[
		\delta(1, y_i, u)=\left\{
		\begin{array}{ll}
		(1 , u )  &\ \text{si} \ a = 1  \\
		(1, y_i \cdot u') &\ \text{si} \  u = y_jw'  \\
		\end{array}
		\right.
		\]
		donde al apilar hacemos la multiplicación $y_i \cdot u$ en el grupo libre $F$.
		
		Consideremos el lenguaje aceptado por pila vacía, esto es
		
		\[
		{\cal L }(M) = \{  y \in Y^* \mid (y,1,1)   \vdash^*  (1, 1, 1)  \}.
		\]
		
		Notemos que ${\cal L }(M) = \text{WP}(F,Y)$ porque justamente en la pila apilamos lo que vamos leyendo de izquierda a derecha y desapilamos cuando leemos el inverso del generador que está en el tope de la pila.
		Esto es que desapilamos cuando un generador aparece después de su inverso en la palabra.
		
		
		%		Dado un grupo libre $F$ veamos cómo construir un automáta $\cal M$ tal que acepta su problema de la palabra. 
		%		
		%		Pensemos un \APND que tenga dos estados. Uno inicial que también va a ser final para poder aceptar la palabra vacía que corresponde al elemento 1 de nuestro grupo y otro estado para las palabras que no están en el problema de la palabra.
		%		Para eso la idea es tener en la pila lo que fuimos leyendo de nuestra palabra hasta el momento visto como un elemento en el grupo. 
		%		Esto es, cada vez que leemos una letra de la palabra ver de multiplicarla como un elemento en el grupo con lo que tenemos en el tope de la pila. 
		%		Eventualmente cuando hayamos recorrido la palabra entera debería quedarnos una palabra en la pila que queremos que sea exactamente $1$ y esto es lo mismo que pedir que sea aceptada por pila vacía. 
		%		Entonces este automáta lo podemos representar de la siguiente manera:	
		%		
		%		\begin{center}
		%			\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=4.5cm,
		%				scale = 1.15,transform shape]
		%				
		%				\node[state,accepting,initial] (1') {$1'$};
		%				\node[state] (1) [right of=1'] {$1$};
		%				
		%				\path (1') edge[bend left]              node {} (1)
		%				(1) edge[bend left]              node {} (1');
		%			\end{tikzpicture}
		%		\end{center}
		%		
		%		
		%		Donde las transiciones del estado 1' a 1 son todas las transiciones en las cuales la letra que estamos por leer no es la inversa de lo que esté al tope de la pila. Por otro lado las transiciones del estado 1 al estado 1' son todas las que lo que leemos es justamente el inverso de lo que está al tope de la pila.
		
		\item 	$\ZZ \times \ZZ$ no es un grupo independiente de contexto.
		Si tomamos los siguientes generadores como monoide $\Sigma = \{ a,b,c \}$ tal que tenemos un morfismo de monoides $\pi: \Sigma^* \to \ZZ \times \ZZ$ dado por $\pi(a)=(1,0), \pi(b)=(0,1), \pi(c)=(-1,-1)$.
		Bajo esta presentación 
		\[
		WP(\ZZ \times \ZZ, \Sigma) = \{ w \in \Sigma^*  : \ \exists n \in \NN, \ |w|_a = |w|_b = |w|_c = n \}.
		\]
		Este lenguaje no es independiente de contexto.
		Para eso usemos el lema del pumping \ref{pumping} para probarlo por contradicción.
		Si fuera \ic debería existir una constante $n \ge 0$ tal que hace valer las hipotesis del lema.
		Consideremos la palabra $w = a^n b^n c^n \in WP(G, \Sigma)$.
		Si tenemos una factorización 
		\[
		uvwxy = a^nb^nc^n
		\]
		si $|vwx| \le n$ implica que no todas las letras aparecen en $vwx$.
		Supongamos que la letra que no aparece es $c$.
		Por otro lado como $|vx| \le 0$ esto nos dice que al menos una letra aparece en la subpalabra $vx$.
		Si tomamos $i=0$ notemos que la palabra $uwy \in WP(G,\Sigma)$ pero esto es una contradicción porque la cantidad de $c$ en esta palabra es mayor que de $a$ o $b$.
		
	\end{enumerate}
	
\end{ej}




%\subsection{Sistemas de reescritura.}
%
%Un \emph{sistema de reescritura} podemos pensarlo como un grafo en el sentido de Serre. 
%En este caso los vértices son los \emph{objetos} mientras que las aristas las llamamos \emph{movimientos}. 
%Si nuestro sistema de reescritura $\Gamma$ tiene un movimiento de un objeto $a$ en otro objeto $b$ diremos que $a$ puede ser \emph{reescrito} a $b$ y lo denotaremos $a \to_{\Gamma} b$, omitiendo aclarar que estamos considerando el sistema $\Gamma$ en todo caso que no sea ambiguo. 
%Un camino en el grafo lo llamaremos una \emph{derivación}.  
%
%Nosotros queremos que estos sistemas de reescritura sean tales que si tomamos una sucesión de movimientos en algún momento se estabilice y en tal caso llamaremos \emph{terminante.} 
%A su vez los objetos tales que no puedan ser modificados por ningún movimiento llamaremos objetos \emph{terminales.} 
%
%Dado un sistema de reescritura $\Gamma$ por medio de la clausura transitiva que denotaremos $\overset{*}{\rightarrow}_{\Gamma}$ obtenemos una relación transitiva sobre los objetos. 
%Nuestro interés justamente va a estar en estudiar las relaciones de equivalencia que surgen de sistemas de reescritura particulares. 
%
%\begin{ej}
%	Considerar como objetos los enteros y como movimiento dividir al número por dos si es posible. Este sistema de reescritura tiene como objetos terminales los números impares.
%\end{ej}
%
%Otra característica que le vamos a pedir a los sistema de reescritura es que sean \emph{confluentes}. 
%Informalmente esto es que a partir de un objeto, si usamos dos derivaciones distintas entonces eventualmente estas derivaciones se encuentran en algún objeto independientemente de qué derivaciones tomemos. 
%Esto es que dadas derivaciones $a \overset{*}{\rightarrow} b, a \overset{*}{\rightarrow} c$ existe un objeto $d$ tal que $b \overset{*}{\rightarrow} d$ y $c \overset{*}{\rightarrow} d$.
%
%En particular los sistemas que nos van a interesar en este trabajo son sistemas tales que son confluentes y terminantes que son conocidos como \emph{Church-Roser}.
%
%\subsection{Sistemas de reescritura de cadenas.}
%En nuestro caso en particular vamos a trabajar con sistemas de reescritura donde los objetos son palabras sobre algún alfabeto $\Sigma$ y los movimientos son reglas del estilo $w \to v$ que interpretamos de la siguiente manera. 
%Si tenemos alguna palabra que tenga como subpalabra a $w$ podemos modificarla por $v$. 
%Esto lo representamos por el triple $(p, w \to v, q)$ donde $p,q$ son las subpalabras que vienen antes de $w$ tales que pueden ser vacías. 
%
%Al sistema de reescritura $\Gamma$ con alfabeto $\Sigma$ y las reglas $\cal R$ lo vamos a denotar por sr$\left< \Sigma, {\cal R} \right>$. Estos sistemas también son llamados \emph{sistemas de Thue}.


\end{document}

